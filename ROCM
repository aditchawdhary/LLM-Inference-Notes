# Installing VLLM on Rocm hardware

- MI 300x1-192gb
- Running https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning/tree/main
- https://docs.vllm.ai/en/v0.6.5/getting_started/amd-installation.html


```
sudo docker run -it \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add video \
  --ipc=host \
  --shm-size 80G \
  rocm/pytorch:rocm6.2_ubuntu20.04_py3.9_pytorch_release_2.3.0
```
